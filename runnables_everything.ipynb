{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO/ijga1hZfvOXWxrJC+K73",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnangithubbd/Generative-AI/blob/main/runnables_everything.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCXhtQqL6joH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community\n",
        "!pip install langchain langchain-google-genai google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser,StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableBranch, RunnableLambda\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "from google import genai\n",
        "import google.generativeai as genai\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain_core.prompt_values import StringPromptValue\n",
        "\n"
      ],
      "metadata": {
        "id": "HEVFCiQY6sDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "def add_one(x: int):\n",
        "  return x+1\n",
        "def mul_two(x):\n",
        "  return x*2\n",
        "runnable1=RunnableLambda(add_one)\n",
        "runnable2=RunnableLambda(mul_two)\n",
        "sequence=runnable1|runnable2\n",
        "sequence.invoke(5)"
      ],
      "metadata": {
        "id": "gZHgWNsSrBvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RunnableLambda\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def add_one(x: int) -> int:\n",
        "    return x + 1\n",
        "\n",
        "def mul_two(x: int) -> int:\n",
        "    return x * 2\n",
        "\n",
        "def mul_three(x: int) -> int:\n",
        "    return x * 3\n",
        "\n",
        "runnable_1 = RunnableLambda(add_one)\n",
        "runnable_2 = RunnableLambda(mul_two)\n",
        "runnable_3 = RunnableLambda(mul_three)\n",
        "\n",
        "sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n",
        "    \"mul_two\": runnable_2,\n",
        "    \"mul_three\": runnable_3,\n",
        "}\n",
        "sequence.invoke(1)\n"
      ],
      "metadata": {
        "id": "Bu6UZDJWrtaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.configure(api_key=\"AIzaSyAYJeTPCIkDaRRG6qd9vp3PBTMSMmRt41A\")\n",
        "\n",
        "model1 = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "kutWYvJZ60Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(input_text):\n",
        "    if isinstance(input_text, StringPromptValue):\n",
        "        input_text = input_text.text  # Extract the string content\n",
        "\n",
        "    response = model1.generate_content([input_text])  # Ensure input is a list\n",
        "    if hasattr(response, \"text\") and response.text:\n",
        "        return response.text\n",
        "    elif hasattr(response, \"candidates\") and response.candidates:\n",
        "        return response.candidates[0].content\n",
        "    else:\n",
        "        return \"Error: No valid response from model.\""
      ],
      "metadata": {
        "id": "e4RAFmWQ632k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Str\n",
        "from langchain.schema.runnable import RunnableSequence, RouterRunnable\n",
        "prompt=PromptTemplate(\n",
        "    template=\"generate 5 intersting topic about {topic}\",\n",
        "    input_variables=['topic']\n",
        ")\n",
        "parser=StrOutputParser()\n",
        "# chain = prompt|call_model|parser # it`s also correct\n",
        "chain=RunnableSequence(prompt,call_model,parser)\n",
        "result=chain.invoke({\"topic\":\"neural network in deep learning\"})"
      ],
      "metadata": {
        "id": "a-gtZY1F66BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g2k181-G7BWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RunnableSequence\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableMap\n",
        "\n",
        "\n",
        "prompt1=PromptTemplate(\n",
        "    template=\"Write a poem about {topic}\",\n",
        "    input_variables=['topic']\n",
        ")\n",
        "parser1=StrOutputParser()\n",
        "\n",
        "prompt2=PromptTemplate(\n",
        "    template=\"explain the poem - {topic} within 3 points\",\n",
        "    input_variables=['topic']\n",
        ")\n",
        "chain= RunnableSequence(prompt1, call_model, parser,prompt2,call_model,parser)"
      ],
      "metadata": {
        "id": "GHyN9ZnU7jQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({\"topic\":\"school\"}))"
      ],
      "metadata": {
        "id": "xqhy1Zjh8-v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parser=StrOutputParser()\n",
        "prompt=PromptTemplate(\n",
        "    template='Write a job about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "joke_gen_chain=RunnableSequence(prompt,call_model,parser)\n",
        "output = joke_gen_chain.invoke({\"topic\":\"developer\"})\n"
      ],
      "metadata": {
        "id": "js9cgYV7DVAG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lambda (function)\n",
        "\n",
        "def word_count(text):\n",
        "  return len(text.split())\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda, RunnablePassthrough\n",
        "parallel_chain=RunnableParallel({\n",
        "    \"job\":RunnablePassthrough(),\n",
        "    \"word_count\":RunnableLambda(word_count)\n",
        "}\n",
        ")\n",
        "final_chain=RunnableSequence(joke_gen_chain,parallel_chain)\n",
        "result=final_chain.invoke({\"topic\":\"developer\"})"
      ],
      "metadata": {
        "id": "49DVWbTEtAcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['word_count'])\n",
        "print(result['job'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SnWXEr_0uJoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel Parallel Parallel Parallel\n",
        "prompt1= PromptTemplate(\n",
        "    template=\"Generate a tweet about {topic}\",\n",
        "    input_topics=['topic']\n",
        ")\n",
        "prompt2=PromptTemplate(\n",
        "    template=\"Generate a Linkedin post about {topic}\",\n",
        "    input_topics=['topic']\n",
        ")\n",
        "parser=StrOutputParser()\n",
        "chain1=RunnableSequence(prompt1,call_model,parser)\n",
        "chain2=RunnableSequence(prompt2,call_model,parser)\n",
        "parallel_chain=RunnableParallel(\n",
        "    {\n",
        "    \"tweet\" : chain1,\n",
        "    \"post\":chain2,\n",
        "    # \"word_count\":RunnableLambda(word_count)\n",
        "    }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "9qUF_ZB4y23v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=parallel_chain.invoke({'topic':'developer'})"
      ],
      "metadata": {
        "id": "wvHm-RVQz69f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['tweet'])\n",
        "print(\"-------------------------------------------------------------------------------\\n\")\n",
        "print(result['post'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BSn1KVpA2QFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result={\n",
        "    \"tweet\":result['tweet'],\n",
        "    \"post\":result['post'],\n",
        "    \"tweet_words\":word_count(result['tweet']),\n",
        "    \"post_words\":word_count(result['post'])\n",
        "}\n",
        "print(result['post_words'])"
      ],
      "metadata": {
        "id": "_j4cff-l2hg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para=RunnableParallel(\n",
        "    {\n",
        "        \"tweet\":RunnablePassthrough(),\n",
        "        \"word_count\":RunnableLambda(word_count)\n",
        "    }\n",
        ")\n",
        "# here is define the tweet promp1.. ok\n",
        "tweet_gen=RunnableSequence(prompt1,call_model,parser)\n",
        "final_seq=RunnableSequence(tweet_gen, para)\n",
        "result=final_seq.invoke({\"topic\":\"developer\"})\n"
      ],
      "metadata": {
        "id": "OlXn15ZN22Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['tweet'])\n",
        "print(result['word_count'])"
      ],
      "metadata": {
        "id": "O_C8cmQ52vPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RunnableBranch\n",
        "\n",
        "report_prompt=PromptTemplate(\n",
        "    template=\"Generate a report about {topic}\",\n",
        "    input_topics=['topic']\n",
        ")\n",
        "sum_prompt=PromptTemplate(\n",
        "    template=\"Summarize the following report {topic} within 200 words\",\n",
        "    input_topics=['topic']\n",
        ")\n",
        "parser=StrOutputParser()\n",
        "report_gen_chain=RunnableSequence(\n",
        "    report_prompt,\n",
        "    call_model,\n",
        "    parser\n",
        ")\n",
        "branch_chain=RunnableBranch(\n",
        "    (lambda x: len(x.split())>500, RunnableSequence(sum_prompt,call_model,parser)),\n",
        "    RunnablePassthrough()\n",
        ")\n",
        "final_chain=RunnableSequence(\n",
        "    report_gen_chain,\n",
        "    branch_chain\n",
        ")\n",
        "\n",
        "result=final_chain.invoke({\"topic\": \"global warming\"})"
      ],
      "metadata": {
        "id": "82J07El96xYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=final_chain.invoke({\"topic\": \"global warming\"})"
      ],
      "metadata": {
        "id": "V2Tmqxkt7_ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(result.split()))"
      ],
      "metadata": {
        "id": "93x68oq18jeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "pmpt=PromptTemplate.from_template(\n",
        "     'In JSON format, give me a list of {topic} and their '\n",
        "    'corresponding names in French, Spanish and in a '\n",
        "    'Cat Language.'\n",
        ")\n",
        "prser=SimpleJsonOutputParser()\n",
        "chn=pmpt|call_model|prser\n",
        "chn.invoke({\"topic\":\"animals\"})"
      ],
      "metadata": {
        "id": "UJyNWDX3DojQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZpnRvHthSnKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from google.generativeai import GenerativeModel\n",
        "from langchain_core.prompt_values import StringPromptValue\n",
        "\n"
      ],
      "metadata": {
        "id": "xu9hrAEhWZ8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke_chain = (\n",
        "    ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "    | call_model\n",
        ")\n",
        "poem_chain = (\n",
        "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
        "    | call_model\n",
        ")\n",
        "\n",
        "runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
        "\n",
        "# Display stream\n",
        "output = {\"joke\": \"\", \"poem\": \"\"}\n",
        "for chunk in runnable.stream({\"topic\": \"bear\"}):\n",
        "    for key in chunk:\n",
        "        output[key] = output[key] + chunk[key].content\n",
        "    print(output)  # noqa: T201\n",
        "output = {key: \"\" for key, _ in runnable.output_schema()}\n",
        "for chunk in runnable.stream({\"topic\": \"bear\"}):\n",
        "    for key in chunk:\n",
        "        output[key] = output[key] + chunk[key].content\n",
        "    print(output)  # noqa: T201"
      ],
      "metadata": {
        "id": "K0W5kbtgWXfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable.invoke({\"topic\":\"cats\"})"
      ],
      "metadata": {
        "id": "pJPseimSX1X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOllama(model='llama2')\n",
        "\n",
        "# Without bind.\n",
        "chain = (\n",
        "    llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
        "# Output is 'One two three four five.'\n",
        "\n",
        "# With bind.\n",
        "chain = (\n",
        "    llm.bind(stop=[\"three\"])\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
        "# Output is 'One two'"
      ],
      "metadata": {
        "id": "UrQix7ofaXIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "id": "xkoG3w49ak3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # Or use another model name\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Rmklq3xManbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode input text\n",
        "input_text = \"hello world in java programming language.\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "gIsS-sRza4j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n"
      ],
      "metadata": {
        "id": "TRi-YgiBbFAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode input text\n",
        "input_text = \"how to print hello world in java progamming language? \"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs, max_length=40, num_return_sequences=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "58ApES7ObSpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Assign the EOS token as the padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Encode input text\n",
        "input_text = \"how to print hello world in java progmming language ?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Generate output with adjusted parameters for creativity\n",
        "output = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=1000 0,  # Allow for more text generation\n",
        "    num_return_sequences=1,\n",
        "    attention_mask=inputs[\"attention_mask\"],  # Ensure attention mask is used\n",
        "    pad_token_id=tokenizer.eos_token_id,  # Handle padding correctly\n",
        "    temperature=0.7,  # Control randomness\n",
        "    top_p=0.9,        # Control diversity of output\n",
        "    top_k=50,         # Limit the number of possible words at each step\n",
        "    no_repeat_ngram_size=2  # Avoid repeating n-grams\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "KrlfMAb1bfTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}