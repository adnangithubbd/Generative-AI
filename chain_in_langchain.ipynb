{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOYCtBSpe20OFaY16TQtzAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnangithubbd/Generative-AI/blob/main/chain_in_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-google-genai google-generativeai\n"
      ],
      "metadata": {
        "id": "8oQ0-0Uxuk9p",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dz2954Euwfan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA5xQtAij6MO"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser,StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableBranch, RunnableLambda\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "from google import genai\n",
        "import google.generativeai as genai\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain_core.prompt_values import StringPromptValue\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.configure(api_key=\"AIzaSyAYJeTPCIkDaRRG6qd9vp3PBTMSMmRt41A\")\n",
        "\n",
        "model1 = genai.GenerativeModel(\"gemini-2.0-flash\")\n"
      ],
      "metadata": {
        "id": "qDbkDH2aSCMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(input_text):\n",
        "    if isinstance(input_text, StringPromptValue):\n",
        "        input_text = input_text.text  # Extract the string content\n",
        "\n",
        "    response = model1.generate_content([input_text])  # Ensure input is a list\n",
        "    if hasattr(response, \"text\") and response.text:\n",
        "        return response.text\n",
        "    elif hasattr(response, \"candidates\") and response.candidates:\n",
        "        return response.candidates[0].content\n",
        "    else:\n",
        "        return \"Error: No valid response from model.\""
      ],
      "metadata": {
        "id": "NgXptdSLS1A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Str\n",
        "prompt=PromptTemplate(\n",
        "    template=\"generate 5 intersting topic about {topic}\",\n",
        "    input_variables=['topic']\n",
        ")\n",
        "parser=StrOutputParser()\n",
        "chain=prompt|call_model|parser\n",
        "result=chain.invoke({\"topic\":\"balck hole\"})"
      ],
      "metadata": {
        "id": "fES-OGvGS7Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5kKVX0M_WDH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***simple_chain***"
      ],
      "metadata": {
        "id": "C4lQIWZldVdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "parser= StrOutputParser()\n",
        "simple_chain=prompt|call_model|parser\n",
        "\n",
        "result=simple_chain.invoke({\"topic\":\"chemistry\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "8aPKPbGvdRKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"generate a detailed report on {topic}\",\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"Generate a 5 line summary for the text {text}\",\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "# result=call_model(prompt1.format(topic=\"chemistry\"))\n",
        "# print(result)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oDx3XDtoWELl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=call_model(prompt1.format(topic=\"chemistry\"))"
      ],
      "metadata": {
        "id": "abtbT3x5bPAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result2=call_model(prompt2.format(text=result))\n",
        "print(result2)"
      ],
      "metadata": {
        "id": "Sa_iclrWXue7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3y8t293cWM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **sequential chain**"
      ],
      "metadata": {
        "id": "6Ei1zbyCbDkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "json_parser=JsonOutputParser()\n",
        "\n",
        "chain_with_json_parser=prompt1|call_model|prompt2|call_model|json_parser\n",
        "#change like this template=\"Generate a 5 line summary for the text {text} in json fromate\",\n",
        "result=chain_with_json_parser.invoke({\"topic\":\"chemistry\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "YPtxaskmY6yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=prompt1|call_model|prompt2|call_model\n",
        "result=chain.invoke({\"topic\":\"chemistry\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "PYZ2NDigYENH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_anthropic"
      ],
      "metadata": {
        "id": "vR14vulXd4fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda"
      ],
      "metadata": {
        "id": "-pftiFHPgGN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the Anthropic model\n",
        "model2 = ChatAnthropic(model_name=\"claude-3-7-sonnet-20250219\")\n",
        "\n",
        "# Define the prompts\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"generate short and simple notes on {topic}\",\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"Generate 5 short questions and answers from the following {text}\",\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template=\"Merge the provided notes and quiz into a single document \\n notes -{notes} and quiz ->{quiz}\",\n",
        "    input_variables=['notes', 'quiz']\n",
        ")\n",
        "\n",
        "# Define the parser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Function to generate output from the model\n",
        "def call_model(input_text):\n",
        "    response = model2.generate([input_text])  # Ensure input is a list\n",
        "    if hasattr(response, \"text\") and response.text:\n",
        "        return response.text\n",
        "    elif hasattr(response, \"candidates\") and response.candidates:\n",
        "        return response.candidates[0].content\n",
        "    else:\n",
        "        return \"Error: No valid response from model.\"\n",
        "\n",
        "# Define individual chains\n",
        "chain1 = prompt1 | RunnableLambda(call_model) | parser\n",
        "chain2 = prompt2 | RunnableLambda(call_model) | parser\n",
        "\n",
        "# Merge chains\n",
        "merge_chain = prompt3 | RunnableLambda(call_model) | parser\n",
        "\n",
        "# Define the parallel execution\n",
        "final_chain = RunnableParallel(notes=chain1, quiz=chain2)\n",
        "\n",
        "# Run the final chain\n",
        "result = final_chain.invoke({\"topic\": \"chemistry\"})\n",
        "# Now merge the results from the parallel execution\n",
        "merged_result = merge_chain.invoke({\"notes\": result['notes'], \"quiz\": result['quiz']})\n",
        "\n",
        "# Print the final result\n",
        "print(merged_result)\n"
      ],
      "metadata": {
        "id": "yw5nfkVHdkrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# parser1=StrOutputParser()\n",
        "\n",
        "# class FeedBack(BaseModel):\n",
        "#     sentiment: Literal['Positive', \"negative\"]= Field(description=\"Give me the sentment of feedback\")\n",
        "\n",
        "# parser2=PydanticOutputParser(pydantic_object=FeedBack)\n",
        "\n",
        "# prompt1=PromptTemplate(\n",
        "#     template=\"Classify the sentce of the following feedback text into positive or negative {feedback} \\n {format_instruction}\",\n",
        "#     input_variables=['feedback'],\n",
        "#     partial_variables={'format_instruction':parser2.get_format_instructions()}\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# llm_runnale=RunnableLambda(call_model)\n",
        "\n",
        "# classifer_chain=prompt1|llm_runnale|parser1\n",
        "\n",
        "# feedback=\"i realy love the product , it works wonderfully\"\n",
        "# # Invoke the chain with the 'text' key in the input dictionary\n",
        "# result = classifer_chain.invoke({\"feedback\": feedback})\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "lK4rablaPncW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BcnI1X-jM6W6"
      }
    }
  ]
}